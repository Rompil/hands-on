{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark_hands_on.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rompil/hands-on/blob/master/PySpark%20Hands_on.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uh9u3iTqMHwb",
        "scrolled": true,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache-mirror.rbc.ru/pub/apache/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz\n",
        "!tar xvzf spark-2.4.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v3It1Ju_0Sdh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.1-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5zXU2COu3pYU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g54MmIfdQpQs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://devdemo.competentum.com/gt_sales_by_pos_month.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GPEwPym-MPIR"
      },
      "cell_type": "markdown",
      "source": [
        "# Практическое введение в PySpark для Data Scientists (Часть 1)\n",
        "\n",
        "---\n",
        "\n",
        "by Roman Pilyugin"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "3GSTuYdTMu6z"
      },
      "cell_type": "markdown",
      "source": [
        "## Содержание\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1. [Немного теории. RDD, Transformation, Action](#theory)\n",
        "2. [Если я уже умею в Python и Pandas](#forSkilledPeople)\n",
        "3. [Сравнение операций Pandas и PySpark](#PandasVsPySpark)\n",
        "    1. [Чтение данных](#csvRead)\n",
        "    3. [Некоторые основные примеры манипуляций с данными](#moreOps)\n",
        "    4. [Визуализация данным](#pics)\n",
        "    5. [Сохранение данных](#storeData)\n",
        "4. [Погружаемся в Machine Learning](#ML)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5q570MiAPyyL"
      },
      "cell_type": "markdown",
      "source": [
        "##  Немного теории. RDD, Transformation, Action <a name =theory></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Kv2hjQnmAFh8"
      },
      "cell_type": "markdown",
      "source": [
        "**RDD** (Resilient Distributed Dataset) - Это фундаментальная абстракция структуры данных в Spark. Ее можно представить в виде неизменяемой коллекции объектов. Сама коллекция может быть размещена по нескольким нодам (отдельным машинам) для параллельной обработки."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "D-Vz9DqLJEr3"
      },
      "cell_type": "markdown",
      "source": [
        "**Transformation** - операция, результатом которой является новый RDD. Таким образром, transformation задает связь ( последовательность) между RDD в виде последовательности преобразований. \n",
        "\n",
        "*Надо отметить, что все операции Transformation выполняются в **Lazy mode**, т.е. пока не вызвать одну из оперций Action, то преобразования не выполняются.*"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "C5xZXgYPaVYB"
      },
      "cell_type": "markdown",
      "source": [
        "### Основные операции Transformation\n",
        "\n",
        "* map(function) — *применяет функцию function к каждому элементу датасета*\n",
        "\n",
        "* .filter(function) — *возвращает все элементы датасета, на которых функция function вернула истинное значение*\n",
        "\n",
        "* .distinct([numTasks]) — *возвращает датасет, который содержит уникальные элементы исходного датасета*\n",
        "\n",
        "**Также стоит отметить об операциях над множествами, смысл которых понятен из названий:**\n",
        "\n",
        "* .union(otherDataset)\n",
        "\n",
        "* .intersection(otherDataset)\n",
        "\n",
        "* .cartesian(otherDataset) — *новый датасет содержит в себе всевозможные пары (A,B), где первый элемент принадлежит исходному датасету, а второй — датасету-аргументу*\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7_qqNHh9JKFQ"
      },
      "cell_type": "markdown",
      "source": [
        "**Action** - операции, результат которых появляется немедленно, в отличии от Transformation. Это могут быть операции для вывода результата вычислений, сохранения в отдельный файл или выгрузка во внешнее хранилище.\n",
        "*Можно сказать, что Action операция опзволяет извлечь данные из RDD.*"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "OZCBrN6kJGHW"
      },
      "cell_type": "markdown",
      "source": [
        "### Основные операции Action\n",
        "\n",
        "\n",
        "* .saveAsTextFile(path) — *сохраняет данные в текстовый файл (в hdfs, на локальную машину или в любую другую поддерживаемую файловую систему — полный список можно посмотреть в документации)*\n",
        "\n",
        "* .collect() — *возвращает элементы датасета в виде массива. Как правило, это применяется в случаях, когда данных в датасете уже мало (применены различные фильтры и преобразования) — и необходима визуализация, либо дополнительный анализ данных, например средствами пакета Pandas*\n",
        "\n",
        "* .take(n) — *возвращает в виде массива первые n элементов датасета*\n",
        "\n",
        "* .count() — *возвращает количество элементов в датасете*\n",
        "\n",
        "* .reduce(function) — *знакомая операция для тех, кто знаком с MapReduce. Из механизма этой операции следует, что функция function (которая принимает на вход 2 аргумента возвращает одно значение) должна быть обязательно коммутативной и ассоциативной*\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gaYz4jUbQVYE"
      },
      "cell_type": "markdown",
      "source": [
        "## Если я уже умею в Python и Pandas <a name=forSkilledPeople></a>\n",
        "\n",
        "<p>Для практикующего специалиста по анализу данных PySpark может быть полезен когда: </p>\n",
        "\n",
        "\n",
        "*   Уже знакомы c Python (Scala, Java, R) и Pandas DataFrame\n",
        "*   Есть возможность развернуть кластер для обработки данных\n",
        "*   Нужно обрабатывать больше объемы данных [^1].\n",
        "\n",
        "[^1]:   Под большими данными понимаются те, что не помещаются в оперативной памяти одного компьютера\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-M22qEyZQsef"
      },
      "cell_type": "markdown",
      "source": [
        "## Сравнение операций Pandas и PySpark <a name=PandasVsPySpark></a>"
      ]
    },
    {
      "metadata": {
        "id": "mdyIB0vRYa0X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Чтение данных<a name=csvRead></a>\n",
        "В PySpark можно читать данные непосредственно из notebook`а\n",
        "\n",
        "```python\n",
        "simpleData = [5,7,1,12,10,25]\n",
        "newRDD = sc.parallelize(localData)\n",
        "```\n",
        "Но чаще всего приходится иметь дело с внешними источниками данных, которые поддерживает Hadoop - HDFS, HBase и т.д.\n",
        "```python\n",
        "alsoRDD = sc.textFile(\"path_to_your_data_file\")\n",
        "```\n",
        "Замечу, что в обоих случаях мы получаем RDD, о котором мы говорили раньше и с ними уже доступны операции Transformation и Action. \n",
        "\n",
        "Чтобы получить что-то более привычное, типа DataFrame можно выполнить:\n"
      ]
    },
    {
      "metadata": {
        "id": "MTB2yH6yUQs8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "instant_df = spark.sparkContext.parallelize([(1, 2, 3, 'a b c'),\\\n",
        "                                     (4, 5, 6, 'd e f'),\\\n",
        "                                     (7, 8, 9, 'g h i')]).toDF(['col1', 'col2', 'col3','col4'])\n",
        "instant_df.show() # Only now all transformations are executed. .show() - Action method"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JJqBqumHU2pi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Или даже так:"
      ]
    },
    {
      "metadata": {
        "id": "iP1pF-H5VWU4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Employee = spark.createDataFrame([('1', 'Joe', '70000', '1'),\\\n",
        "                                  ('2', 'Henry', '80000', '2'),\\\n",
        "                                  ('3', 'Sam', '60000', '2'),\\\n",
        "                                  ('4', 'Max', '90000', '1')],\\\n",
        "                                 ['Id', 'Name', 'Sallary','DepartmentId'])\n",
        "Employee.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IOURu3MWUdYE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "После первого знакомства с DataFrame в PySpark мы мможем сравнить с DataFrame в Pandas.\n",
        "\n",
        "Ну и для первого примера рассмотрим чтение данных из CSV-файла."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "a6tCfVQ29LCS"
      },
      "cell_type": "markdown",
      "source": [
        "__Как это в Pandas__\n",
        "```python\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/FileStore/tables/gt_sales_by_pos_month.csv')\n",
        "```\n",
        "Конечно, есть дополнительные параметры для детальнойго указания параметров CSV-файла и я их опустил."
      ]
    },
    {
      "metadata": {
        "id": "8PZ_5y-CYa0b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__В PySpark__"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UW-l3wk5_S5q",
        "scrolled": false,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = spark.read.options(header=True).options(inferSchema=True).csv('gt_sales_by_pos_month.csv')\n",
        "# the first line of files are used to name columns and are not included in data.\n",
        "# automatically infer column types. It requires one extra pass over the data and is false by default."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ZH8fJzqXS9I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Подробнее про чение файлов и возможные опции в [документации](https://docs.databricks.com/spark/latest/data-sources/read-csv.html#reading-files) "
      ]
    },
    {
      "metadata": {
        "id": "8J1290lEqvc6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Более того, DataFrame во многом схожа с таблицей в базе данных и можно посмотреть её схему:"
      ]
    },
    {
      "metadata": {
        "id": "jgTS0neQrSko",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cBqLWndgYa0i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Манипуляции с данными <a name=manipulation></a>"
      ]
    },
    {
      "metadata": {
        "id": "422_EVeRYa0k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Взглянем на результат предыдущей операции.\n",
        "Для этого можем посмотреть на первые строки нашего DataFrame.\n",
        "\n",
        "__Как это в Pandas__\n",
        "```python\n",
        "df.head(5)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "NerFtCZoYa0m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__В PySpark__"
      ]
    },
    {
      "metadata": {
        "id": "I_oHGU6_Ya0o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7bG4ZvWPf2bI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gv-jTF7SYa0r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "В остальном, некоторые операции практически идентичны."
      ]
    },
    {
      "metadata": {
        "id": "EhYQPRtBYa0s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.columns # узнаем про столбцы в таблице"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uzbn4qQZ7D3-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.dtypes # узнаем про типы данных в таблице"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UZUebkVb58bl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.describe().show() # .show() needs to be call to show results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d2ntgsPhUZ4j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Примечание 1:** В pandas и PySpark метод .describe() по-разному считает  стандартное отклонение. В первом случае несмещенная , а во втором смещенная оценка стандартного отклонения."
      ]
    },
    {
      "metadata": {
        "id": "-HU6N4uiug7e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Выбор данных.**\n",
        "\n",
        "При работы с данными в основном манипулируют столбцами.\n",
        "\n",
        "Посмотреть отдельный столбец или даже несколько можно так:"
      ]
    },
    {
      "metadata": {
        "id": "g5g6K6CsOkW8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.select('sales_date').distinct().show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qiHKTqhbOu3L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.select(['brand_manufacturer', 'brand_variant']).distinct().count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LXsRXYMFOD7d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Примечание 2:** *.distinct()* - возвращает неупорядоченную коллекцию уникальных элементов.\n",
        "\n",
        "**Примечание 3:** *.count()* - поведение похоже на метов .count() в pandas, но отличается. Если в pandas результат равен количеству не NA/null элементов, то в PySpark оно равно просто количеству строк в заданном DataFrame."
      ]
    },
    {
      "metadata": {
        "id": "mpUMZq78Ya0z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Фильтрация__\n",
        "\n",
        "В PySpark DataFrame нет индексов для строк и выбор строк осуществляется по заданию определенных условий- фильтров. Это работает аналогично тому, как в pandas.Dataframe."
      ]
    },
    {
      "metadata": {
        "id": "Fo2Q9TdaYa00",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df[df.meg_mrsp > 200].distinct().count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9OpZf-FchTHr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Таким образом можно формировать сложные уловия выбора:"
      ]
    },
    {
      "metadata": {
        "id": "ZAFKsGrPYa03",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df[(df.meg_mrsp > 85)&(df.brand_variant=='JADE LA ROSE 100 SSL')].count() # to know how many items satisfy the condition"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RxTyHTbLYa0u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Операции со столбцами. **\n",
        "\n",
        "__Удаление столбцов__\n",
        "\n",
        "Может понадобиться убрать некоторые столбцы из таблицы т.к. они нам не пригодятся в дальнейшем.\n",
        "*order_number* - отличный кандидат для этого.\n",
        "\n",
        "__В Pandas__\n",
        "```python\n",
        "df.drop('order_number', axis=1)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "DivMx1TEYa0v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__В PySpark__"
      ]
    },
    {
      "metadata": {
        "id": "cEDFrUgEYa0w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.drop('order_number').show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ArKVZgOOYa0y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "В PySpark нет понятия индекса и axis. Вам не надо указывать явную ось."
      ]
    },
    {
      "metadata": {
        "id": "hW4MKoTNsvWL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Добавление столбцов__.\n",
        "\n",
        "DataFrame неизменяемы т.е. нельзя изменить содержимое столбца, но можно создать новый DataFrame.\n",
        "\n",
        "**В  Pandas:**\n",
        "```python\n",
        "df['new_column'] = 'default value'\n",
        "```\n",
        "Конечно, в реальной задаче будет не так просто, это просто пример.\n",
        "\n",
        "**В PySpark:**"
      ]
    },
    {
      "metadata": {
        "id": "dxzH-eb7hBHd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.withColumn('Dummy_Column', df.sales_volume_sticks * 2 ).show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TWKfxQbgi7pL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Возможна ситуация, когда применить изменения нужно \"на месте\" (in-place) - неизменяя заголовок столбца, хотя df неизменяемый объект."
      ]
    },
    {
      "metadata": {
        "id": "8a93UEREjbHf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.withColumn('Dummy_Column', df.sales_volume_sticks * 2 ).drop('sales_volume_sticks').withColumnRenamed('Dummy_Column','sales_volume_sticks').show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8kRXm-Rkp3nf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Перегруппировка данных.**\n",
        "\n",
        "Аналогична методу в pandas, отличий особых нет. \n",
        "\n",
        "**В Pandas**\n",
        "```python\n",
        "df.groupby('brand_manufacturer').agg({'sales_volume_sticks':'sum','meg_mrsp':'count'})\n",
        "```\n",
        "**В PySpark:**"
      ]
    },
    {
      "metadata": {
        "id": "mW7dEIdOqEr2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.groupby('brand_manufacturer').agg({'sales_volume_sticks':'sum','meg_mrsp':'count'}).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T18fIjoAyXXK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Стоит отдельно сказать про метод .agg() - для стандартных операций аггрегации проблем нет, но в PySpark нет пока возможности для использования кастомных (пользоветельских) функций и приходится использовать втроенный набор: 'sum', 'min', 'max', 'count' и 'mean'."
      ]
    },
    {
      "metadata": {
        "id": "woouVWFK1QiM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.groupby('brand_manufacturer').min('meg_mrsp').show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Vz8K5ET7RlY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Конечно, тем небольшим набором функций возможности PySpark не ограничиваются. Есть отдлельный модуль pyspark.sql.functions c наиболее частовстречающимися функциями. ПОдробнее об этих функция в [документации](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)."
      ]
    },
    {
      "metadata": {
        "id": "SydxTIph8nI5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "print(dir(F))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VTsmjWxx-ihG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Теперь, зная о таком модуле мы можем переписать предыдущие примеры по другому."
      ]
    },
    {
      "metadata": {
        "id": "osKPFgsS-wWK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.groupby('brand_manufacturer').agg(F.min('meg_mrsp').alias('min_meg_mrsp'),\\\n",
        "                                    F.max('meg_mrsp').alias('max_meg_mrsp'),\\\n",
        "                                    F.min('sales_volume_sticks').alias('min_sales_volume_sticks'),\\\n",
        "                                    F.max('sales_volume_sticks').alias('max_sales_volume_sticks')).show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Ns8RJtIDdQs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Основной причиной почему стоит пользоваться именно встроенными функциями вместо самописных состоит в том,  что они написаны не на python, в на scala и выполняются нативным кодом на JVM с распределенными данными на нескольких нодах. Безусловно, такой код производительнее по сравнению с обычными питоновскими функциями.\n",
        "\n",
        "Но может возникнуть ситуация, что стандартными функциями не обойтись. В этом случае можно воспользоваться User Defined Function (UDF)\n",
        "\n",
        "Определим функцию, которую потом применим к столбцу."
      ]
    },
    {
      "metadata": {
        "id": "kuGqyVhkF3A7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_negative_values(number):\n",
        "  '''\n",
        "  This function remove negative values with zeros\n",
        "  \n",
        "  '''\n",
        "  return number if number > 0 else 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UJlD4wa7H93W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "В Pandas могло быть сделано так:\n",
        "```python\n",
        "pandasF = lambda row: remove_negative_values(row.sales_volume_sticks)\n",
        "df.apply(pandasF, axis=1)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "FFZo9cofG-KM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sparkF = F.udf(remove_negative_values, 'float')\n",
        "df.select(sparkF(df.sales_volume_sticks).alias('result')).show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jCbN1HbxJxXa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Отчистка данных является одним и необходимых этапов в процессе приготовления данных перед непосредсвенных их анализом. Удаление строк с пропущенными значениями или замена значений в них на другие  довольно стандартные операции и не могло быть так, что для них не реализовали соответствующие методы.\n"
      ]
    },
    {
      "metadata": {
        "id": "3T3Wesz3X3AF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.fillna({'sales_volume_sticks':0.0, 'meg_mrsp':0.0 })\n",
        "df.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JNeOWPhDiBva",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Можно выбросить еще все дублирующие друг друга строки:"
      ]
    },
    {
      "metadata": {
        "id": "26OZ4E4miNjA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.dropDuplicates()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Da-vm4vWiRkH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Но может так случиться, что вам понадобится добавлять данные в DataFrame. На этот случай есть специальный метод .unionAll(). Схожий по назначению метод в pandas .concat(), но результат у нах может быть разный. \n",
        "\n",
        "Пример, когда два DataFrame  имеют отлиную друг от друга scheme, но все равно происходит слияние, даже  не возникает ошибки. Но если будет разное число столбцов, то ошибка возникнет, что отличается от поведения .concat() в pandas, где нераспознаные столбцы будут добавлены в итоговый DataFrame, а недостающие значения будут заменены на NaN."
      ]
    },
    {
      "metadata": {
        "id": "e_BBFt-Kbygu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_instant_df = spark.sparkContext.parallelize([(1, 2, 3, 'a b c'),\\\n",
        "                                     (4, 5, 6, 'd e f'),\\\n",
        "                                     (42, 8, 9, 1)]).toDF(['col1', 'col2', 'col3','col5'])\n",
        "new_instant_df.show()\n",
        "instant_df.unionAll(new_instant_df).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nfGi1abJ2XUi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Приведение типов данных**\n",
        "\n",
        "После того, как представление о структуре данных были получены можно посмотреть какой тип данных был выбран для каждого из столбцов в DataFrame, если ни один не подходит, то тип по-умолчанию - string. Иногда приходится явным образов преобразовывать содержимое одной ячейки к другому типу."
      ]
    },
    {
      "metadata": {
        "id": "Eqd52-cfbO-Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from pyspark.sql.types import Integertype\n",
        "df.withColumn('int_meg_mrsp', F.col('meg_mrsp').cast('int')).drop('meg_mrsp').withColumnRenamed('int_meg_mrsp','meg_mrsp').show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cIWy2LufDv3v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Так же часто может понадобиться упорядочивать DataFrame по возрастанию или убыванию вдоль заданной колонки.\n",
        "**В Pandas:**\n",
        "```python\n",
        "df.sort_value('meg_mrsp', ascending=True) # sorting from low to high\n",
        "```\n",
        "**В PySpark:**"
      ]
    },
    {
      "metadata": {
        "id": "UZB7zzaAJ3Gm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.sort('meg_mrsp',ascending=True).select('meg_mrsp').distinct().show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x54QlG8kKkal",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.orderBy(['brand_manufacturer','brand_variant'], ascending=[0,1]).distinct().show(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Px_iqD2VL_gH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Если вам все же привычнее работать с pandas.DataFrame, то для этих случаев есть замечательный метод .toPandas().\n",
        "Тут нужно еще упомянуть такой проект как [Apache Arrow](https://arrow.apache.org/), который позводяет оптимизировать работу  с pandas и numpy в PySpark. При этом, эта функция  по-умолчанию отключена. Чтобы включить ее надо просто установить флаг в True. Хотя это пока не вышло за рамки эксперимента.\n",
        "```python\n",
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.execution.arrow.fallback.enabled\", \"true\")\n",
        "```\n",
        "Еще одна важная вещь, о которой нужно помнить при преобразовании в pandas.DataFrame, это то, почему приходится использовать PySpark - большие объемы данных. Неосмотрительный вызов  .toPandas() может значительно снизить, а то и просто повесить, компьютер из-за слишком большого объема данных в оперативной памяти.\n",
        "Хорошей практикой считается предварительно взглянуть на данные, что планируются преобразовать в pandas.DataFrame"
      ]
    },
    {
      "metadata": {
        "id": "glAXB7tOTG4A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.execution.arrow.fallback.enabled\", \"true\")\n",
        "df.show(5)\n",
        "pdf = df.select('*').toPandas()\n",
        "pdf.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oRZkSWhEYoWi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "На это возможности для оптимизации не заканчиваются. В основном оптимизация касается работой с память, ведь как говорилось ранее, что все методы Transformation сами по себе не выполняются, а лишь строят граф преобразований исходных данных, который выполняется при вызове одного из методов Action. Может так случиться, что вызывая несколько раз методы Action, мы затставляем раз за разом выполнять лдни и теже довольно трудоемкие задачи. Этого можно избежать, единажды закэшировав результат. Наприемр сразу после чтения, после всех промежуточных преобразований или перед записью данных в файл. \n",
        "\n",
        "Для освобождения памяти можно вызвать метод .unpersist()"
      ]
    },
    {
      "metadata": {
        "id": "B9hygIT5cI_8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%timeit df.sort('meg_mrsp',ascending=True).select('meg_mrsp').distinct().count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-xftMfE5VRZY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "df = df.cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e6xHHG15b_BQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%timeit df.sort('meg_mrsp',ascending=True).select('meg_mrsp').distinct().count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hkAhxEHxftqn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Когда уже все манипуляции с данными совершины, то самое время записать их в файл или другое внешнее хранилище.\n",
        " Это не создает обычно никаких сложностей:"
      ]
    },
    {
      "metadata": {
        "id": "v4Lx6p39irCo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.write.csv('as_original_file.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DsUOGtVIjnsw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ".csv() - это Action метод, который вызывает цепочку преобразований.\n",
        "Можно последовательно записать все преобразования в строчку - это называется pipeline, хоя это термин нам еще встретится дальше."
      ]
    },
    {
      "metadata": {
        "id": "Zpuc7S06j5Fw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "spark.read.options(header=True)\\\n",
        ".options(inferSchema=True)\\\n",
        ".csv('gt_sales_by_pos_month.csv')\\\n",
        ".dropDuplicates()\\\n",
        ".fillna({'sales_volume_sticks':0.0, 'meg_mrsp':0.0 })\\\n",
        ".withColumn('int_meg_mrsp', F.col('meg_mrsp').cast('int')).drop('meg_mrsp').withColumnRenamed('int_meg_mrsp','meg_mrsp')\\\n",
        ".orderBy(['brand_manufacturer','brand_variant'], ascending=[0,1])\\\n",
        ".coalesce(1).write.format(\"com.databricks.spark.csv\")\\\n",
        ".option(\"header\", \"true\").csv('modified_example1.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0y8YXnxMpwDr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Но такой способ хранение результатов не самый эффективный, все данные для записи в один файл передаются в один едиственный worker, который и осуществляет запись."
      ]
    },
    {
      "metadata": {
        "id": "qcNcht2RtdtP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Визуализация.\n",
        "\n",
        "Для отображения результатов обработки и представления данных удобно использовать различные графики, но в самом PySpark встроенных инструментов для этого нет и приходится прибегать к сторонним модулям. Самый простой способ - это конвертировать DataFrame в pandas.DataFrame и уже использовтаь стандартные средства отображения (Matplotlib, seaborn или Bokeh).\n",
        "\n",
        "Предполагается, что вы работаете с PySpark в Databricks и для отображения данных использовать можно функцию display(), которую можно настроить под конкретные требования к графику.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yPXrkZlyiUJW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "display(df)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VZnAllp0vkMR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "display(df.take(5))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}