{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark_hands_on.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rompil/hands-on/blob/master/PySpark%20Hands_on.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uh9u3iTqMHwb",
        "scrolled": true,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache-mirror.rbc.ru/pub/apache/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz\n",
        "!tar xvzf spark-2.4.1-bin-hadoop2.7.tgz.1\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v3It1Ju_0Sdh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.1-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5zXU2COu3pYU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GPEwPym-MPIR"
      },
      "cell_type": "markdown",
      "source": [
        "# Практическое введение в PySpark для Data Scientists\n",
        "\n",
        "---\n",
        "\n",
        "by Roman Pilyugin"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "3GSTuYdTMu6z"
      },
      "cell_type": "markdown",
      "source": [
        "## Содержание\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1. [Немного теории. RDD, Transformation, Action](#theory)\n",
        "2. [Если я уже умею в Python и Pandas](#forSkilledPeople)\n",
        "3. [Сравнение операций Pandas и PySpark](#PandasVsPySpark)\n",
        "    1. [Чтение данных](#csvRead)\n",
        "    3. [Некоторые основные примеры манипуляций с данными](#moreOps)\n",
        "    4. [Визуализация данным](#pics)\n",
        "    5. [Сохранение данных](#storeData)\n",
        "4. [Погружаемся в Machine Learning](#ML)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5q570MiAPyyL"
      },
      "cell_type": "markdown",
      "source": [
        "##  Немного теории. RDD, Transformation, Action <a name =theory></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Kv2hjQnmAFh8"
      },
      "cell_type": "markdown",
      "source": [
        "**RDD** (Resilient Distributed Dataset) - Это фундаментальная абстракция структуры данных в Spark. Ее можно представить в виде неизменяемой коллекции объектов. Сама коллекция может быть размещена по нескольким нодам (отдельным машинам) для параллельной обработки."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "D-Vz9DqLJEr3"
      },
      "cell_type": "markdown",
      "source": [
        "**Transformation** - операция, результатом которой является новый RDD. Таким образром, transformation задает связь ( последовательность) между RDD в виде последовательности преобразований. \n",
        "\n",
        "*Надо отметить, что все операции Transformation выполняются в **Lazy mode**, т.е. пока не вызвать одну из оперций Action, то преобразования не выполняются.*"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "C5xZXgYPaVYB"
      },
      "cell_type": "markdown",
      "source": [
        "### Основные операции Transformation\n",
        "\n",
        "* map(function) — *применяет функцию function к каждому элементу датасета*\n",
        "\n",
        "* .filter(function) — *возвращает все элементы датасета, на которых функция function вернула истинное значение*\n",
        "\n",
        "* .distinct([numTasks]) — *возвращает датасет, который содержит уникальные элементы исходного датасета*\n",
        "\n",
        "**Также стоит отметить об операциях над множествами, смысл которых понятен из названий:**\n",
        "\n",
        "* .union(otherDataset)\n",
        "\n",
        "* .intersection(otherDataset)\n",
        "\n",
        "* .cartesian(otherDataset) — *новый датасет содержит в себе всевозможные пары (A,B), где первый элемент принадлежит исходному датасету, а второй — датасету-аргументу*\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7_qqNHh9JKFQ"
      },
      "cell_type": "markdown",
      "source": [
        "**Action** - операции, результат которых появляется немедленно, в отличии от Transformation. Это могут быть операции для вывода результата вычислений, сохранения в отдельный файл или выгрузка во внешнее хранилище.\n",
        "*Можно сказать, что Action операция опзволяет извлечь данные из RDD.*"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "OZCBrN6kJGHW"
      },
      "cell_type": "markdown",
      "source": [
        "### Основные операции Action\n",
        "\n",
        "\n",
        "* .saveAsTextFile(path) — *сохраняет данные в текстовый файл (в hdfs, на локальную машину или в любую другую поддерживаемую файловую систему — полный список можно посмотреть в документации)*\n",
        "\n",
        "* .collect() — *возвращает элементы датасета в виде массива. Как правило, это применяется в случаях, когда данных в датасете уже мало (применены различные фильтры и преобразования) — и необходима визуализация, либо дополнительный анализ данных, например средствами пакета Pandas*\n",
        "\n",
        "* .take(n) — *возвращает в виде массива первые n элементов датасета*\n",
        "\n",
        "* .count() — *возвращает количество элементов в датасете*\n",
        "\n",
        "* .reduce(function) — *знакомая операция для тех, кто знаком с MapReduce. Из механизма этой операции следует, что функция function (которая принимает на вход 2 аргумента возвращает одно значение) должна быть обязательно коммутативной и ассоциативной*\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gaYz4jUbQVYE"
      },
      "cell_type": "markdown",
      "source": [
        "## Если я уже умею в Python и Pandas <a name=forSkilledPeople></a>\n",
        "\n",
        "<p>Для практикующего специалиста по анализу данных PySpark может быть полезен когда: </p>\n",
        "\n",
        "\n",
        "*   Уже знакомы c Python (Scala, Java, R) и Pandas DataFrame\n",
        "*   Есть возможность развернуть кластер для обработки данных\n",
        "*   Нужно обрабатывать больше объемы данных [^1].\n",
        "\n",
        "[^1]:   Под большими данными понимаются те, что не помещаются в оперативной памяти одного компьютера\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-M22qEyZQsef"
      },
      "cell_type": "markdown",
      "source": [
        "## Сравнение операций Pandas и PySpark <a name=PandasVsPySpark></a>"
      ]
    },
    {
      "metadata": {
        "id": "mdyIB0vRYa0X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Чтение данных<a name=csvRead></a>\n",
        "В PySpark можно читать данные непосредственно из notebook`а\n",
        "\n",
        "```python\n",
        "simpleData = [5,7,1,12,10,25]\n",
        "newRDD = sc.parallelize(localData)\n",
        "```\n",
        "Но чаще всего приходится иметь дело с внешними источниками данных, которые поддерживает Hadoop - HDFS, HBase и т.д.\n",
        "```python\n",
        "alsoRDD = sc.textFile(\"path_to_your_data_file\")\n",
        "```\n",
        "Замечу, что в обоих случаях мы получаем RDD, о котором мы говорили раньше и с ними уже доступны операции Transformation и Action. \n",
        "\n",
        "Чтобы получить что-то более привычное, типа DataFrame можно выполнить:\n"
      ]
    },
    {
      "metadata": {
        "id": "MTB2yH6yUQs8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "instant_df = spark.sparkContext.parallelize([(1, 2, 3, 'a b c'),\\\n",
        "                                     (4, 5, 6, 'd e f'),\\\n",
        "                                     (7, 8, 9, 'g h i')]).toDF(['col1', 'col2', 'col3','col4'])\n",
        "instant_df.show() # Only now all transformations are executed. .show() - Action method"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JJqBqumHU2pi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Или даже так:"
      ]
    },
    {
      "metadata": {
        "id": "iP1pF-H5VWU4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Employee = spark.createDataFrame([('1', 'Joe', '70000', '1'),\\\n",
        "                                  ('2', 'Henry', '80000', '2'),\\\n",
        "                                  ('3', 'Sam', '60000', '2'),\\\n",
        "                                  ('4', 'Max', '90000', '1')],\\\n",
        "                                 ['Id', 'Name', 'Sallary','DepartmentId'])\n",
        "Employee.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IOURu3MWUdYE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "После первого знакомства с DataFrame в PySpark мы мможем сравнить с DataFrame в Pandas.\n",
        "\n",
        "Ну и для первого примера рассмотрим чтение данных из CSV-файла."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "a6tCfVQ29LCS"
      },
      "cell_type": "markdown",
      "source": [
        "__Как это в Pandas__\n",
        "```python\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/FileStore/tables/gt_sales_by_pos_month.csv')\n",
        "```\n",
        "Конечно, есть дополнительные параметры для детальнойго указания параметров CSV-файла и я их опустил."
      ]
    },
    {
      "metadata": {
        "id": "8PZ_5y-CYa0b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__В PySpark__"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UW-l3wk5_S5q",
        "scrolled": false,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = spark.read.options(header=True)\\ #  the first line of files are used to name columns and are not included in data.\n",
        "                .options(inferSchema=True)\\ # automatically infer column types. It requires one extra pass over the data and is false by default.\n",
        "                .csv('gt_sales_by_pos_month.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ZH8fJzqXS9I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Подробнее про чение файлов и возможные опции в [документации](https://docs.databricks.com/spark/latest/data-sources/read-csv.html#reading-files) "
      ]
    },
    {
      "metadata": {
        "id": "8J1290lEqvc6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Более того, DataFrame во многом схожа с таблицей в базе данных и можно посмотреть её схему:"
      ]
    },
    {
      "metadata": {
        "id": "jgTS0neQrSko",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cBqLWndgYa0i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Манипуляции с данными <a name=manipulation></a>"
      ]
    },
    {
      "metadata": {
        "id": "422_EVeRYa0k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Взглянем на результат предыдущей операции.\n",
        "Для этого можем посмотреть на первые строки нашего DataFrame.\n",
        "\n",
        "__Как это в Pandas__\n",
        "```python\n",
        "df.head(5)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "NerFtCZoYa0m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__В PySpark__"
      ]
    },
    {
      "metadata": {
        "id": "I_oHGU6_Ya0o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7bG4ZvWPf2bI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gv-jTF7SYa0r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "В остальном, некоторые операции практически идентичны."
      ]
    },
    {
      "metadata": {
        "id": "EhYQPRtBYa0s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.columns # узнаем про столбцы в таблице"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uzbn4qQZ7D3-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.dtypes # узнаем про типы данных в таблице"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UZUebkVb58bl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.describe().show() # .show() needs to be call to show results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-HU6N4uiug7e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Выбор данных.**\n",
        "\n",
        "В PySpark DataFrame нет индексов для строк и выбор строк осуществляется по заданию определенных условий- фильтров. Этоработает аналогично тому, как в pandas.Dataframe."
      ]
    },
    {
      "metadata": {
        "id": "mpUMZq78Ya0z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Фильтрация__\n",
        "Для выбора данных из Dataframe можно использовать операции сравнения, аналогично тому, как с pandas.dataframe."
      ]
    },
    {
      "metadata": {
        "id": "Fo2Q9TdaYa00",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df[df.meg_mrsp > 150].describe().show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9OpZf-FchTHr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Таким образом можно формировать сложные уловия выбора:"
      ]
    },
    {
      "metadata": {
        "id": "ZAFKsGrPYa03",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df[(df.meg_mrsp > 85)&(df.brand_variant=='JADE LA ROSE 100 SSL')].count() # to know how many items satisfy the condition"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RxTyHTbLYa0u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Операции со столбцами. **\n",
        "\n",
        "__Удаление столбцов__\n",
        "\n",
        "Может понадобиться убрать некоторые столбцы из таблицы т.к. они нам не пригодятся в дальнейшем.\n",
        "*order_number* - отличный кандидат для этого.\n",
        "\n",
        "__В Pandas__\n",
        "```python\n",
        "df.drop('order_number', axis=1)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "DivMx1TEYa0v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__В PySpark__"
      ]
    },
    {
      "metadata": {
        "id": "cEDFrUgEYa0w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.drop('order_number').show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ArKVZgOOYa0y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "В PySpark нет понятия индекса и axis. Вам не надо указывать явную ось."
      ]
    },
    {
      "metadata": {
        "id": "hW4MKoTNsvWL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Добавление столбцов__.\n",
        "\n",
        "DataFrame неизменяемы т.е. нельзя изменить содержимое столбца, но можно создать новый DataFrame."
      ]
    },
    {
      "metadata": {
        "id": "14fOuVY6qh6c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.select('brand_manufacturer').distinct().show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}