{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark ML.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rompil/hands-on/blob/master/PySpark_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "JHU8JLE21Dym",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Обзор модуля PySpark MLlib\n"
      ]
    },
    {
      "metadata": {
        "id": "njVqKQZNAr55",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Модуль MLlib предназначен для использования уже готовых алгоритмов машинного обучения в прикладных задачах при обработке больших данных. Во многом он схож с Scikit-learn, что делает его легким в освоении и позволяет переносить уже готовые методы обработки данных на платформу Spark."
      ]
    },
    {
      "metadata": {
        "id": "OTReAhKhBeUW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Для лучшего понимания того, зачем делать определенные этапы потом, мы обсудим задачу машинного обучения  в первом приближении.\n",
        "\n",
        "Вне зависимости от конкретного выбранного алгоритма анализа данных можно выделить схожие этапы:\n",
        "\n",
        "\n",
        "*   Загрузка и отчистка данных.\n",
        "*   Извлечение признаков (фич)\n",
        "*   Тренировка модели\n",
        "*   Оценка целевых показателей\n",
        "\n",
        "Эти этапы могут повторяться неоднократно в процессе настройки гиперпараметров модели.\n",
        "\n",
        "В Spark такой подход унифицировали и сосокупность таких этапов упаковали в **Pipeline**.\n",
        "Каждый этап в Pipeline представляет из себя либо Transformer, либо Estimator, либо Evaluator.\n",
        "При этом, Pipeline может содержать произвольное количество таких унафицированных этапов, каждый из которых может быть соединен с другим и выступать как източником , так и потребителем для других этапов.\n",
        "Каждый из таких этапов надо рассмотреть подробнее.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "QzvSJVGERx-T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Transformer:** \n",
        "\n",
        "\n",
        "*   Используется на этапе препроцессинга\n",
        "*   Преобразует данные в необходимый формат для далдьнейшей обработки\n",
        "*   Обрабатывает по столбцам столбец на входе -> столбец на выходе\n",
        "\n",
        "Примеры Transformers:\n",
        "          \n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}